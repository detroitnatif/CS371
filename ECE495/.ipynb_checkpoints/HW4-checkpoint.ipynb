{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe1105fb-a342-45c0-ab74-0b02b91b02d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets\n",
    "# !pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddc2c417-287a-4a0d-855e-a352b74147fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3d7ca4e-5d58-449f-a92e-3c9907ae3f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████| 16800/16800 [00:32<00:00, 516.05 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 30522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment = load_dataset(\"yelp_polarity\")\n",
    "tr = sentiment['train']\n",
    "ts = sentiment['test']\n",
    "tr = tr.train_test_split(test_size=0.97)['train']\n",
    "ts = ts.train_test_split(test_size=0.97)['train']\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_function(data):\n",
    "    return tokenizer(data['text'], padding='max_length', truncation=True)\n",
    "\n",
    "tokenized_dataset = tr.map(tokenize_function, batched=True)\n",
    "\n",
    "word_count = tokenizer.vocab_size\n",
    "print(\"Vocabulary Size:\", word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5f86443-8819-44d3-ba9e-e716dc6410b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████| 16800/16800 [00:29<00:00, 574.66 examples/s]\n",
      "Map: 100%|██████████████████████████| 1140/1140 [00:02<00:00, 470.57 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tr = tr.map(tokenize_function, batched=True)\n",
    "tr.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "tr_loader = DataLoader(tr, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "ts = ts.map(tokenize_function, batched=True)\n",
    "ts.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "ts_loader = DataLoader(ts, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "54a2cb4d-0b23-4691-8cfd-ad1a0118b2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, word_count, embedding_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.embed_words = nn.Embedding(word_count, embedding_dim)  \n",
    "        self.layer1 = nn.Linear(embedding_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embed_words(x) \n",
    "        avg_embedding = embedded.mean(dim=1) \n",
    "        out = self.layer1(avg_embedding) \n",
    "        return out  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ec9e5f1c-166a-4b05-87b1-a585c4b46473",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0773acd0-fcd7-4710-877b-ab98eccc506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(word_count, embedding_dim)\n",
    "stepper = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_function = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a51afc58-e762-4786-bb4a-10c71ed5faec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(model, dataloader, stepper, loss_function, iterations=10):\n",
    "    model.train()\n",
    "    for e in range(iterations):\n",
    "        total_loss = 0\n",
    "        for sample in dataloader:\n",
    "            inputs, attention_mask, labels = sample['input_ids'], sample['attention_mask'], sample['label']\n",
    "            stepper.zero_grad()\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = loss_function(outputs, labels.float())\n",
    "            loss.backward()\n",
    "            stepper.step()\n",
    "            total_loss = total_loss + loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b493282f-ffd5-4348-a303-d3da0d3a6982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.669491346081098\n",
      "Epoch 2, Loss: 0.6069448996299789\n",
      "Epoch 3, Loss: 0.5227746870546114\n",
      "Epoch 4, Loss: 0.4488695376047066\n",
      "Epoch 5, Loss: 0.39134810373896645\n",
      "Epoch 6, Loss: 0.34697813470803557\n",
      "Epoch 7, Loss: 0.31305692597514106\n",
      "Epoch 8, Loss: 0.2858913141063281\n",
      "Epoch 9, Loss: 0.26447297050307195\n",
      "Epoch 10, Loss: 0.24689224763019454\n"
     ]
    }
   ],
   "source": [
    "forward_pass(model, tr_loader, stepper, loss_function, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "202afe1e-d58b-4d47-80fb-c58b8527f1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sample in dataloader:\n",
    "            inputs, attention_mask, labels = sample['input_ids'], sample['attention_mask'], sample['label']\n",
    "            outputs = model(inputs).squeeze()\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "613363ea-52a4-4772-bfda-797be70cbbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 88.95%\n"
     ]
    }
   ],
   "source": [
    "test_model(model, ts_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455741bd-f44c-4319-8c32-ecdd539acab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34a7b15-5cf9-4637-ba47-2e4299a512c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c218fc-f788-4546-84ee-95ba31aa5388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c5d249-a23e-4274-8d14-206b4c128ceb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21277eaf-ab1d-4886-9a74-f62cba289eea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d741feca-01f6-4189-8592-2f06b93a4058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e4cfea2-d94a-48e2-8937-a2177062d9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = load_dataset(\"yelp_polarity\")\n",
    "tr = sentiment['train']\n",
    "ts = sentiment['test']\n",
    "tr = tr.train_test_split(test_size=0.97)['train']\n",
    "ts = ts.train_test_split(test_size=0.97)['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f39e610-7ba3-48f4-aa9e-0cdb4b0d4b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/compsci371/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f871e2f2-4c8f-458a-aeec-e1979e063a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d8c8765-25bd-4d64-a019-cb7a1013b69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(batch):\n",
    "    return tokenizer(batch['text'], padding=True, truncation=True, max_length=512, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29e86025-1597-4cc6-8669-2b97334bfeea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████| 16800/16800 [00:08<00:00, 1869.18 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tr = tr.map(tokenize_function, batched=True)\n",
    "tr.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a427a92-cb5d-45b8-9233-bbac958cc1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(tr, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8a0869a-011e-4389-a7d6-f3d36e4b2cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████| 1140/1140 [00:00<00:00, 1930.57 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ts = ts.map(tokenize_function, batched=True)\n",
    "ts.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "ts_loader = DataLoader(ts, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ecdffff-d53f-4aa9-bb80-9fb926554dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/compsci371/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c62111-d855-4af3-abe3-ea0847df1ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1): \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f71a92-a37a-4e3e-83d3-40c643bddbea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7640d90-4b77-4279-a85e-354de178abfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

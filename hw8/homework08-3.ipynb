{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "r\"\"\"",
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "AST"
    ]
   },
   "source": [
    "<style>\n",
    "    @media print{\n",
    "        body {\n",
    "            position:relative !important;\n",
    "        }\n",
    "        .celltag_new_page {\n",
    "            page-break-before: always !important;\n",
    "        }\n",
    "    }\n",
    "</style>\n",
    "<div hidden>\n",
    "    $$\n",
    "    \\newcommand{\\reals}{\\mathbb{R}}\n",
    "    \\newcommand{\\naturals}{\\mathbb{N}}\n",
    "    \\newcommand{\\integers}{\\mathbb{Z}}\n",
    "    \\newcommand{\\prob}{\\mathbb{P}}\n",
    "    \\newcommand{\\expect}{\\mathbb{E}}\n",
    "    \\newcommand{\\b}[1]{\\mathbf{#1}}\n",
    "    \\newcommand{\\bs}[1]{\\boldsymbol{#1}}\n",
    "    \\newcommand{\\c}[1]{\\mathcal{#1}}\n",
    "    \\newcommand{mat}[2]{\\left[\\,\\begin{array}{#1}#2\\end{array}\\,\\right]}\n",
    "    $$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "AST"
    ]
   },
   "source": [
    "# COMPSCI 371 Homework 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "T"
    ]
   },
   "source": [
    "_**Group Members:**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "tags": [
     "AT"
    ]
   },
   "source": [
    "### Problem 0 (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "AST"
    ]
   },
   "source": [
    "## Part 1: MLP Back-Propagation: Mathematics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 1.1 (Exam Style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 1.2 (Exam Style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 1.3 (Exam Style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 1.4 (Exam Style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "r\"\"\"",
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "AT"
    ]
   },
   "source": [
    "\\begin{eqnarray*}\n",
    "\\b{g}_{V_2} &=& \\b{g}_{\\b{z}} \\cdot J_{V_2} \\\\\n",
    "\\b{g}_{\\b{b}_2} &=& \\\\\n",
    "\\b{g}_{\\b{q}} &=& \n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 1.5 (Exam Style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "r\"\"\"",
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "T"
    ]
   },
   "source": [
    "Forward pass:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\b{x} &=&  \\\\\n",
    "\\b{p} &=&  \\\\\n",
    "\\b{q} &=&  \\\\\n",
    "\\b{z} &=&  \\\\\n",
    "\\lambda &=& \n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "r\"\"\"",
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "T"
    ]
   },
   "source": [
    "Local Jacobians:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "J_{V_2} &=&  \\\\\n",
    "J_{\\b{b}_2} &=&  \\\\\n",
    "J_{\\b{q}} &=&  \\\\\n",
    "J_{\\b{p}} &=&  \\\\\n",
    "J_{V_1} &=&  \\\\\n",
    "J_{\\b{b}_1} &=& \n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "r\"\"\"",
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "T"
    ]
   },
   "source": [
    "Gradients:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\b{g}_{\\b{z}} &=&  \\\\\n",
    "\\b{g}_{V_2} &=&  \\\\\n",
    "\\b{g}_{\\b{b}_2} &=&  \\\\\n",
    "\\b{g}_{\\b{q}} &=&  \\\\\n",
    "\\b{g}_{\\b{p}} &=&  \\\\\n",
    "\\b{g}_{V_1} &=&  \\\\\n",
    "\\b{g}_{\\b{b}_1} &=& \n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "AST"
    ]
   },
   "source": [
    "## Part 2: MLP Back-Propagation: Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/student/Desktop/CS371/myenv/lib/python3.13/site-packages (2.1.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "AST"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from types import SimpleNamespace\n",
    "\n",
    "tests = [\n",
    "    SimpleNamespace(\n",
    "        v1=np.array(((2, -1), (0, 3), (2, 1)), dtype=float),\n",
    "        b1=np.array((2, -1, 3), dtype=float),\n",
    "        v2=np.array(((2, 1, -3), (3, -1, 2)), dtype=float),\n",
    "        b2=np.array((4, 2), dtype=float),\n",
    "        x=np.array((-1, 2), dtype=float),\n",
    "        y=np.array((-4, 1), dtype=float),\n",
    "        loss='quadratic'\n",
    "    ),\n",
    "    SimpleNamespace(\n",
    "        v1=np.array(((2, -1, 0), (0, 1, 3)), dtype=float),\n",
    "        b1=np.array((2, -1), dtype=float),\n",
    "        v2=np.array((2, -3), dtype=float),\n",
    "        b2=np.array(4, dtype=float),\n",
    "        x=np.array((-1, 2, 0), dtype=float),\n",
    "        y=np.array(3, dtype=float),\n",
    "        loss='quadratic'\n",
    "    ),\n",
    "    SimpleNamespace(\n",
    "        v1=np.array(((1, 2), (1, 0)), dtype=float),\n",
    "        b1=np.array((-5, 2), dtype=float),\n",
    "        v2=np.array(((2, -3), (1, 0), (-1, -1)), dtype=float),\n",
    "        b2=np.array((1, 3, -2), dtype=float),\n",
    "        x=np.array((-1, 2), dtype=float),\n",
    "        y=np.array(2, dtype=int),\n",
    "        loss='hinge'\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "AST"
    ]
   },
   "outputs": [],
   "source": [
    "def array_string(a):\n",
    "    if np.isscalar(a) or a.ndim == 0:\n",
    "        return '{:.4g}'.format(a)\n",
    "    return '(' + ', '.join([array_string(x) for x in a]) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "AST"
    ]
   },
   "outputs": [],
   "source": [
    "def dot(a, b):\n",
    "    return a * b if np.isscalar(a) or np.isscalar(b)\\\n",
    "        else np.einsum('...i,i...', a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmt_array(d):\n",
    "    if np.isscalar(d) or d.ndim == 0:\n",
    "        if float(d).is_integer():\n",
    "            return str(int(d))\n",
    "        else:\n",
    "            return '{:.4g}'.format(d)\n",
    "    return '(' + ', '.join([fmt_array(r) for r in d]) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def multi_dot(x, y):\n",
    "    return x * y if np.isscalar(x) or np.isscalar(y) else np.einsum('...i,i...', x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_affine(M, c):\n",
    "    return {\n",
    "        'layer_type': 'affine',\n",
    "        'weights': M,\n",
    "        'bias': c,\n",
    "        'store_input': True,\n",
    "        'input_val': None,\n",
    "        'grad_weights': None,\n",
    "        'grad_bias': None\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def affine_forward(lay, val):\n",
    "    if lay['store_input']:\n",
    "        lay['input_val'] = val.copy()\n",
    "    return np.dot(lay['weights'], val) + lay['bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_backward(lay, incoming_grad):\n",
    "    inp = lay['input_val']\n",
    "    \n",
    "\n",
    "    if np.isscalar(incoming_grad):\n",
    "        incoming_grad = np.array([incoming_grad])\n",
    "    if np.isscalar(inp):\n",
    "        inp = np.array([inp])\n",
    "  \n",
    "    lay['grad_weights'] = np.outer(incoming_grad, inp) if lay['weights'].ndim > 1 else incoming_grad * inp\n",
    "    lay['grad_bias'] = incoming_grad\n",
    " \n",
    "    if lay['weights'].ndim == 1:\n",
    "        return lay['weights'] * incoming_grad\n",
    "    else:\n",
    "        return np.dot(lay['weights'].T, incoming_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_relu():\n",
    "    return {\n",
    "        'layer_type': 'relu',\n",
    "        'store_input': True,\n",
    "        'input_val': None\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_forward(lay, val):\n",
    "    if lay['store_input']:\n",
    "        lay['input_val'] = val.copy()\n",
    "    return np.maximum(0, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def relu_backward(lay, incoming_grad):\n",
    "    inp = lay['input_val']\n",
    "    return incoming_grad * (inp > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quad_loss_forward(tgt, pred):\n",
    "    diff = pred - tgt\n",
    "    return 0.5 * np.dot(diff, diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quad_loss_backward(tgt, pred):\n",
    "    return (pred - tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_loss_forward(tgt, pred):\n",
    "    s = np.log(np.sum(np.exp(pred)))\n",
    "    return s - pred[int(tgt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_loss_backward(tgt, pred):\n",
    "    exp_preds = np.exp(pred)\n",
    "    sm = exp_preds / np.sum(exp_preds)\n",
    "    grads = sm.copy()\n",
    "    grads[int(tgt)] -= 1.0\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network(layers, loss_type, decision_func=None):\n",
    "    return {\n",
    "        'layers': layers,\n",
    "        'loss_type': loss_type,\n",
    "        'decision_func': decision_func,\n",
    "        'store_input': True,\n",
    "        'loss_value': None,\n",
    "        'net_input': None,\n",
    "        'prediction': None,\n",
    "        'layer_grads': []\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_to_inference(net):\n",
    "    net['store_input'] = False\n",
    "    for lay in net['layers']:\n",
    "        lay['store_input'] = False\n",
    "        lay['input_val'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_to_training(net):\n",
    "    net['store_input'] = True\n",
    "    for lay in net['layers']:\n",
    "        lay['store_input'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_layer(lay, val):\n",
    "    if lay['layer_type'] == 'affine':\n",
    "        return affine_forward(lay, val)\n",
    "    elif lay['layer_type'] == 'relu':\n",
    "        return relu_forward(lay, val)\n",
    "    else:\n",
    "        raise ValueError(\"Unrecognized layer type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def backward_layer(lay, grad_val):\n",
    "    if lay['layer_type'] == 'affine':\n",
    "        return affine_backward(lay, grad_val)\n",
    "    elif lay['layer_type'] == 'relu':\n",
    "        return relu_backward(lay, grad_val)\n",
    "    else:\n",
    "        raise ValueError(\"Unrecognized layer type\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(net, val, target=None):\n",
    "    net['net_input'] = val\n",
    "    if target is None:\n",
    "        switch_to_inference(net)\n",
    "        activation_res = val\n",
    "        for lay in net['layers']:\n",
    "            activation_res = forward_layer(lay, activation_res)\n",
    "        if net['decision_func'] is not None:\n",
    "            return net['decision_func'](activation_res)\n",
    "        else:\n",
    "            return activation_res\n",
    "    else:\n",
    "        switch_to_training(net)\n",
    "        activation_res = val\n",
    "        for lay in net['layers']:\n",
    "            activation_res = forward_layer(lay, activation_res)\n",
    "        net['prediction'] = activation_res\n",
    "        if net['loss_type'] == 'quadratic':\n",
    "            net['loss_value'] = quad_loss_forward(target, activation_res)\n",
    "        else:\n",
    "            net['loss_value'] = hinge_loss_forward(target, activation_res)\n",
    "        return net['loss_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(net, target):\n",
    "    if net['loss_type'] == 'quadratic':\n",
    "        init_grad = quad_loss_backward(target, net['prediction'])\n",
    "    else:\n",
    "        init_grad = hinge_loss_backward(target, net['prediction'])\n",
    "    for lay in reversed(net['layers']):\n",
    "        init_grad = backward_layer(lay, init_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_gradients(net):\n",
    "    grads = []\n",
    "    for lay in net['layers']:\n",
    "        if lay['layer_type'] == 'affine':\n",
    "            grads.append((lay['grad_weights'], lay['grad_bias']))\n",
    "        else:\n",
    "            grads.append(None)\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_layers(test_case):\n",
    "   \n",
    "    if isinstance(test_case.b2, np.ndarray):\n",
    "        return [\n",
    "            init_affine(test_case.v1, test_case.b1),\n",
    "            init_relu(),\n",
    "            init_affine(test_case.v2, test_case.b2)\n",
    "        ]\n",
    "    else:\n",
    "        # Ensure b2 is a 1D array for consistency\n",
    "        return [\n",
    "            init_affine(test_case.v1, test_case.b1),\n",
    "            init_relu(),\n",
    "            init_affine(test_case.v2.reshape((1, -1)), np.array([test_case.b2]))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_network(layer_defs, loss_type):\n",
    "    \"\"\"Set up the network with the specified layers and loss function.\"\"\"\n",
    "    loss_function = 'quadratic' if loss_type == 'quadratic' else 'hinge'\n",
    "    decision_func = None if loss_type == 'quadratic' else np.argmax\n",
    "    return create_network(layer_defs, loss_function, decision_func=decision_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_forward_pass(network, layer_defs, x_input):\n",
    "    \"\"\"Compute the forward pass values and store them in a dictionary.\"\"\"\n",
    "    forward_vals = {\n",
    "        'x_inp': x_input,\n",
    "        'layer1_out': affine_forward(layer_defs[0], x_input),\n",
    "        'layer2_out': relu_forward(layer_defs[1], affine_forward(layer_defs[0], x_input)),\n",
    "        'final_out': affine_forward(layer_defs[2], relu_forward(layer_defs[1], affine_forward(layer_defs[0], x_input))),\n",
    "        'loss': network['loss_value']\n",
    "    }\n",
    "    return forward_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_local_jacobians(layer_defs, forward_vals):\n",
    "    jacobian_V2_shape = (\n",
    "        layer_defs[2]['weights'].shape[0], \n",
    "        layer_defs[2]['weights'].shape[0], \n",
    "        layer_defs[2]['weights'].shape[1] if layer_defs[2]['weights'].ndim > 1 else 1\n",
    "    )\n",
    "    jacobian_V1_shape = (\n",
    "        layer_defs[0]['weights'].shape[0], \n",
    "        layer_defs[0]['weights'].shape[0], \n",
    "        layer_defs[0]['weights'].shape[1] if layer_defs[0]['weights'].ndim > 1 else 1\n",
    "    )\n",
    "    \n",
    "    bias_b2_shape = layer_defs[2]['bias'].shape[0] if layer_defs[2]['bias'].ndim > 0 else 1\n",
    "    bias_b1_shape = layer_defs[0]['bias'].shape[0] if layer_defs[0]['bias'].ndim > 0 else 1\n",
    "    \n",
    "    local_jacobs = {\n",
    "        'jacobian_V2': np.zeros(jacobian_V2_shape),\n",
    "        'jacobian_b2': np.eye(bias_b2_shape),\n",
    "        'jacobian_layer2': layer_defs[2]['weights'],\n",
    "        'jacobian_layer1_out': np.diag((forward_vals['layer1_out'] > 0).astype(int)),\n",
    "        'jacobian_V1': np.zeros(jacobian_V1_shape),\n",
    "        'jacobian_b1': np.eye(bias_b1_shape)\n",
    "    }\n",
    "    return local_jacobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradients(network, test_case, layer_defs):\n",
    "    if network['loss_type'] == 'quadratic':\n",
    "        grad_final_out = quad_loss_backward(test_case.y, network['prediction'])\n",
    "    else:\n",
    "        grad_final_out = hinge_loss_backward(test_case.y, network['prediction'])\n",
    "    \n",
    "    grad_layer2_inp = affine_backward(layer_defs[2], grad_final_out)\n",
    "    grad_layer1_out = relu_backward(layer_defs[1], grad_layer2_inp)\n",
    "    x_gradient = affine_backward(layer_defs[0], grad_layer1_out)\n",
    "    \n",
    "    gradients = gather_gradients(network)\n",
    "    w_grad1, b_grad1 = gradients[0] if gradients[0] is not None else (None, None)\n",
    "    w_grad2, b_grad2 = gradients[2] if gradients[2] is not None else (None, None)\n",
    "    \n",
    "    grad_vals = {\n",
    "        'grad_final_out': grad_final_out,\n",
    "        'grad_w2': w_grad2,\n",
    "        'grad_b2': b_grad2,\n",
    "        'grad_layer2_inp': grad_layer2_inp,\n",
    "        'grad_layer1_out': grad_layer1_out,\n",
    "        'grad_w1': w_grad1,\n",
    "        'grad_b1': b_grad1\n",
    "    }\n",
    "    return grad_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_test(test_case, net_idx):\n",
    "    layer_defs = initialize_layers(test_case)\n",
    "    network = setup_network(layer_defs, test_case.loss)\n",
    "    \n",
    "    net_loss = forward_pass(network, test_case.x, test_case.y)\n",
    "    forward_vals = compute_forward_pass(network, layer_defs, test_case.x)\n",
    "    \n",
    "\n",
    "    local_jacobs = calculate_local_jacobians(layer_defs, forward_vals)\n",
    "    \n",
    "    # Backward pass and gradient calculation\n",
    "    backward_pass(network, test_case.y)\n",
    "    grad_vals = calculate_gradients(network, test_case, layer_defs)\n",
    "    \n",
    "    return forward_vals, local_jacobs, grad_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_test_results(net_index, forward_data, jacobians, gradients):\n",
    "\n",
    "    print(f\"Network {net_index}\")\n",
    "    print(\"Forward Pass:\")\n",
    "    print(f\"    Input x: {fmt_array(forward_data['x_inp'])}\")\n",
    "    print(f\"    Layer 1 Output (p): {fmt_array(forward_data['layer1_out'])}\")\n",
    "    print(f\"    Layer 2 Output (q): {fmt_array(forward_data['layer2_out'])}\")\n",
    "    print(f\"    Final Output (z): {fmt_array(forward_data['final_out'])}\")\n",
    "    print(f\"    Loss (lambda): {fmt_array(forward_data['loss'])}\")\n",
    "\n",
    "    print(\"Local Jacobians:\")\n",
    "    print(f\"    J_V2: {fmt_array(jacobians['jacobian_V2'])}\")\n",
    "    print(f\"    J_b2: {fmt_array(jacobians['jacobian_b2'])}\")\n",
    "    print(f\"    J_q: {fmt_array(jacobians['jacobian_layer2'])}\")\n",
    "    print(f\"    J_p: {fmt_array(jacobians['jacobian_layer1_out'])}\")\n",
    "    print(f\"    J_V1: {fmt_array(jacobians['jacobian_V1'])}\")\n",
    "    print(f\"    J_b1: {fmt_array(jacobians['jacobian_b1'])}\")\n",
    "    \n",
    "    print(\"Gradients:\")\n",
    "    print(f\"    g_z: {fmt_array(gradients['grad_final_out'])}\")\n",
    "    print(f\"    g_V2: {fmt_array(gradients['grad_w2'])}\")\n",
    "    print(f\"    g_b2: {fmt_array(gradients['grad_b2'])}\")\n",
    "    print(f\"    g_q: {fmt_array(gradients['grad_layer2_inp'])}\")\n",
    "    print(f\"    g_p: {fmt_array(gradients['grad_layer1_out'])}\")\n",
    "    print(f\"    g_V1: {fmt_array(gradients['grad_w1'])}\")\n",
    "    print(f\"    g_b1: {fmt_array(gradients['grad_b1'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network 0\n",
      "Forward Pass:\n",
      "    Input x: (-1, 2)\n",
      "    Layer 1 Output (p): (-2, 5, 3)\n",
      "    Layer 2 Output (q): (0, 5, 3)\n",
      "    Final Output (z): (0, 3)\n",
      "    Loss (lambda): 10\n",
      "Local Jacobians:\n",
      "    J_V2: (((0, 0, 0), (0, 0, 0)), ((0, 0, 0), (0, 0, 0)))\n",
      "    J_b2: ((1, 0), (0, 1))\n",
      "    J_q: ((2, 1, -3), (3, -1, 2))\n",
      "    J_p: ((0, 0, 0), (0, 1, 0), (0, 0, 1))\n",
      "    J_V1: (((0, 0), (0, 0), (0, 0)), ((0, 0), (0, 0), (0, 0)), ((0, 0), (0, 0), (0, 0)))\n",
      "    J_b1: ((1, 0, 0), (0, 1, 0), (0, 0, 1))\n",
      "Gradients:\n",
      "    g_z: (4, 2)\n",
      "    g_V2: ((0, 20, 12), (0, 10, 6))\n",
      "    g_b2: (4, 2)\n",
      "    g_q: (14, 2, -8)\n",
      "    g_p: (0, 2, -8)\n",
      "    g_V1: ((0, 0), (-2, 4), (8, -16))\n",
      "    g_b1: (0, 2, -8)\n",
      "\n",
      "Network 1\n",
      "Forward Pass:\n",
      "    Input x: (-1, 2, 0)\n",
      "    Layer 1 Output (p): (-2, 1)\n",
      "    Layer 2 Output (q): (0, 1)\n",
      "    Final Output (z): 1\n",
      "    Loss (lambda): 2\n",
      "Local Jacobians:\n",
      "    J_V2: (((0), (0)), ((0), (0)))\n",
      "    J_b2: ((1))\n",
      "    J_q: (2, -3)\n",
      "    J_p: ((0, 0), (0, 1))\n",
      "    J_V1: (((0, 0, 0), (0, 0, 0)), ((0, 0, 0), (0, 0, 0)))\n",
      "    J_b1: ((1, 0), (0, 1))\n",
      "Gradients:\n",
      "    g_z: -2\n",
      "    g_V2: (0, -2)\n",
      "    g_b2: (-2)\n",
      "    g_q: (-4, 6)\n",
      "    g_p: (0, 6)\n",
      "    g_V1: ((0, 0, 0), (-6, 12, 0))\n",
      "    g_b1: (0, 6)\n",
      "\n",
      "Network 2\n",
      "Forward Pass:\n",
      "    Input x: (-1, 2)\n",
      "    Layer 1 Output (p): (-2, 1)\n",
      "    Layer 2 Output (q): (0, 1)\n",
      "    Final Output (z): (-2, 3, -3)\n",
      "    Loss (lambda): 6.009\n",
      "Local Jacobians:\n",
      "    J_V2: (((0, 0), (0, 0), (0, 0)), ((0, 0), (0, 0), (0, 0)), ((0, 0), (0, 0), (0, 0)))\n",
      "    J_b2: ((1, 0, 0), (0, 1, 0), (0, 0, 1))\n",
      "    J_q: ((2, -3), (1, 0), (-1, -1))\n",
      "    J_p: ((0, 0), (0, 1))\n",
      "    J_V1: (((0, 0), (0, 0)), ((0, 0), (0, 0)))\n",
      "    J_b1: ((1, 0), (0, 1))\n",
      "Gradients:\n",
      "    g_z: (0.006676, 0.9909, -0.9975)\n",
      "    g_V2: ((0, 0.006676), (0, 0.9909), (0, -0.9975))\n",
      "    g_b2: (0.006676, 0.9909, -0.9975)\n",
      "    g_q: (2.002, 0.9775)\n",
      "    g_p: (0, 0.9775)\n",
      "    g_V1: ((0, 0), (-0.9775, 1.955))\n",
      "    g_b1: (0, 0.9775)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, case in enumerate(test_cases):\n",
    "    forward_data, local_jacobians, gradients = execute_test(case, index)\n",
    "    show_test_results(index, forward_data, local_jacobians, gradients)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "myenv311",
   "language": "python",
   "name": "myenv311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
